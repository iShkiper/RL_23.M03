{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b618a1fa-154e-445c-adb0-5341bd014dbf",
   "metadata": {},
   "source": [
    "# Фреймворк для RL и новые методы\n",
    "\n",
    "Рассмотрим работу методов DDPG, TD3, SAC реализованных в библиотеке `stable_baselines3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5bd64b-51be-48bf-8df7-f8e8edf5a4cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "stable_baselines3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ae322-f836-46af-a6b2-3387264ae2ec",
   "metadata": {},
   "source": [
    "Сравним работу алгоритмов DDPG, TD3 на примере среды `Pendulum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dddef0-003c-497e-83d0-750a78ca0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG, TD3, SAC\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefde494-7760-4792-89ea-a0ca5de6b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_DDPG = os.path.join('savedModels', 'SB3_DDPG_Pendulum')\n",
    "log_path = os.path.join('logs', 'SB3_DDPG_TD3_Pendulum')\n",
    "save_path_TD3 = os.path.join('savedModels', 'SB3_TD3_Pendulum')\n",
    "\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "model_DDPG = DDPG('MlpPolicy', env,  tensorboard_log=log_path)\n",
    "model_TD3 = TD3('MlpPolicy', env,  tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2f939a7-14ce-438f-bd23-872f8d285d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=\"C:\\Users\\AlexK\\Documents\\Python Scripts\\RL\\logs\\SB3_DDPG_TD3_Pendulum\"\n"
     ]
    }
   ],
   "source": [
    "full_logs_path = os.path.join(os.getcwd() ,log_path)\n",
    "print(\"\".join((\"tensorboard --logdir=\",'\"',full_logs_path,'\"')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1668e0d6-db15-4035-874c-3124982c9265",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71bcc77a06e42b98158b08510b0a41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x1f98c0bd9a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DDPG.learn(total_timesteps=30000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0249836-846b-4595-b9e1-fdafd710d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DDPG.save(save_path_DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b8d092e-1cd0-48e0-a896-db8cb27191fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d8d3e2434a4d72865e1d1c69bf5382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.td3.td3.TD3 at 0x1f9868d3fb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TD3.learn(total_timesteps=30000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5671c39-a18e-4054-a2b6-44a33c8f1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_TD3.save(save_path_TD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2b95a9b-5cfd-4683-b67f-4eae8e687888",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DDPG = DDPG.load(save_path_DDPG, env=env)\n",
    "model_TD3 = TD3.load(save_path_TD3, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba533722-37c4-4e66-81a2-7f088c56b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний доход:  -139.8770171607705\n",
      "Стандартное отклонение:  70.22451910749129\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model_DDPG, env, n_eval_episodes=100, render=False)\n",
    "\n",
    "print(\"Средний доход: \", mean_reward)\n",
    "print(\"Стандартное отклонение: \", std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79a3ee57-0206-427a-bd45-3ffe44681b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний доход:  -141.01297798662404\n",
      "Стандартное отклонение:  70.56964080995978\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model_TD3, env, n_eval_episodes=100, render=False)\n",
    "\n",
    "print(\"Средний доход: \", mean_reward)\n",
    "print(\"Стандартное отклонение: \", std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a623f2-9ea9-4fee-8ba9-570167aa6ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c956c48e-423e-4a17-9dfc-da0d1d2a3ec7",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3360e-fe5c-4985-a26a-97d61c19a3cc",
   "metadata": {},
   "source": [
    "Рассмотрим среду `Humanoid-v5`, которая моделирует шагающего человека. См. подробности по ссылке:\n",
    "\n",
    "https://gymnasium.farama.org/environments/mujoco/humanoid/\n",
    "\n",
    "Цель в том, чтобы научить человечка двигаться как можно дольше. Состояние - это вектор размерности 347, действия - это 17-мерный вектор.\n",
    "\n",
    "Запустим симуляцию со случайными действями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2b62621-18bd-4f97-bfb6-8a63f6678806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2da3fa1-654d-487a-a317-fcac5f0f4747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний доход:  141.45276010607796\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Humanoid-v5', render_mode = 'human')\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, 1)\n",
    "state, _ = env.reset()\n",
    "gain = 0\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    gain += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"Средний доход: \", np.mean(env.return_queue))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48718f1f-6efa-45bc-a45d-25ab9dc6c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "save_path = os.path.join('savedModels', 'SB3_SAC_Humanoid')\n",
    "save_path_best = os.path.join('savedModels', 'SB3_SAC_Humanoid')\n",
    "log_path = os.path.join('logs', 'SB3_SAC_Humanoid')\n",
    "\n",
    "\n",
    "vec_env = make_vec_env(\"Humanoid-v5\", n_envs=4)\n",
    "\n",
    "# создание callback\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=4000, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=20000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f33c21cb-2247-42d2-adf5-9b751aca2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = make_vec_env(\"Humanoid-v5\", n_envs=4)\n",
    "model = SAC('MlpPolicy', vec_env, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d6fa105-bd6b-408a-87f3-ac6eb98128c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 348]               0\n",
      "  FlattenExtractor-2                  [-1, 348]               0\n",
      "            Linear-3                  [-1, 256]          89,344\n",
      "              ReLU-4                  [-1, 256]               0\n",
      "            Linear-5                  [-1, 256]          65,792\n",
      "              ReLU-6                  [-1, 256]               0\n",
      "            Linear-7                   [-1, 17]           4,369\n",
      "            Linear-8                   [-1, 17]           4,369\n",
      "             Actor-9                   [-1, 17]               0\n",
      "================================================================\n",
      "Total params: 163,874\n",
      "Trainable params: 163,874\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.63\n",
      "Estimated Total Size (MB): 0.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.policy, input_size=(1,348,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "058479f5-415e-4caf-8652-a106b82d212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=\"C:\\Users\\AlexK\\Documents\\Python Scripts\\RL\\logs\\SB3_SAC_Humanoid\"\n"
     ]
    }
   ],
   "source": [
    "full_logs_path = os.path.join(os.getcwd() ,log_path)\n",
    "print(\"\".join((\"tensorboard --logdir=\",'\"',full_logs_path,'\"')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da49994a-de06-4b9c-bcd6-5d0c1f163fa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db16edbdef5414a94011245b3cf6bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1f998e5c380>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1500, callback=eval_callback, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c035f0a-68a6-4151-a261-ecc0bf5ef778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94cb53c3-c512-4a4d-8d8a-78bff7f8898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Humanoid-v5')\n",
    "load_path = os.path.join(save_path_best, 'best_model')\n",
    "model = SAC.load(load_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a62c5e-ba18-4a0b-bb58-6e429eeafbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v5')\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, render=False)\n",
    "print(mean_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93b088fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v5', render_mode = 'human')\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1, render=True)\n",
    "print(mean_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0152a-8d8f-44ef-a149-e6bf27e2aeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
